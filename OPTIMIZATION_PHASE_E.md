# FastResize - Phase E: Performance Optimization
## Má»¥c tiÃªu: ÄÃ¡nh báº¡i libvips trong batch processing

---

## ğŸ“Š HIá»†N TRáº NG

### Váº¥n Ä‘á»
- **libvips batch nhanh hÆ¡n FastResize** trong benchmark
- FastResize chá»‰ lÃ m resize nhÆ°ng cháº­m hÆ¡n libvips (thÆ° viá»‡n to, Ä‘a nÄƒng)
- Äiá»u nÃ y KHÃ”NG THá»‚ CHáº¤P NHáº¬N Ä‘Æ°á»£c

### Benchmark hiá»‡n táº¡i
```bash
cd /Users/canh.th/Desktop/fastgems/fastresize
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build
./build/benchmark/bench_vs_libvips
```

### Root Cause Analysis

**Bottleneck chÃ­nh Ä‘Ã£ phÃ¡t hiá»‡n:**

1. **PNG Encoding - Global Mutex Bottleneck** ğŸ”´ CRITICAL
   - File: `src/encoder.cpp:133`
   - Váº¥n Ä‘á»: `std::lock_guard<std::mutex> lock(png_encode_mutex);`
   - Impact: PNG encoding hoÃ n toÃ n SERIAL, khÃ´ng thá»ƒ parallel
   - Máº¥t mÃ¡t: ~60% performance cho PNG batch

2. **JPEG Encoding - Suboptimal** ğŸ”´ HIGH
   - File: `src/encoder.cpp:109-112`
   - Váº¥n Ä‘á»: Encode tá»«ng scanline má»™t, khÃ´ng táº­n dá»¥ng libjpeg-turbo tá»‘i Ä‘a
   - Impact: JPEG encoding cháº­m hÆ¡n 20-30% so vá»›i potential

3. **Memory Allocation trong Hot Path** ğŸŸ  MEDIUM
   - File: `src/encoder.cpp:76-90`
   - Váº¥n Ä‘á»: Má»—i láº§n RGBAâ†’RGB allocate buffer má»›i
   - Impact: 15-20% overhead cho images cÃ³ alpha channel

4. **Pipeline Threshold quÃ¡ cao** ğŸŸ  MEDIUM
   - File: `src/fastresize.cpp:270`
   - Váº¥n Ä‘á»: Pipeline chá»‰ activate vá»›i batch â‰¥50
   - Impact: Batch nhá» khÃ´ng Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« pipeline architecture

---

## ğŸ¯ Káº¾ HOáº CH Tá»I Æ¯U (3 PHASES)

### Tá»•ng quan chiáº¿n lÆ°á»£c
```
Phase E1: Quick Wins        â†’ Expected: 50-70% faster  (2-3 days)
Phase E2: Deep Optimization â†’ Expected: +40-50% faster (3-5 days)
Phase E3: Advanced Tuning   â†’ Expected: +10-20% faster (1 week)

Tá»•ng cá»™ng: 2-3x nhanh hÆ¡n hiá»‡n táº¡i
```

---

## ğŸ“‹ PHASE E1: QUICK WINS (High Impact, Low Risk)

### Má»¥c tiÃªu
- **50-70% faster** so vá»›i hiá»‡n táº¡i
- **100% backward compatible**
- KhÃ´ng thay Ä‘á»•i API, output giá»‘ng há»‡t

### E1.1: Remove PNG Global Mutex (Impact: 60% faster PNG)

**Files cáº§n sá»­a:**
- `src/encoder.cpp:130-200`

**Hiá»‡n táº¡i:**
```cpp
bool encode_png(...) {
    std::lock_guard<std::mutex> lock(png_encode_mutex);  // â† BOTTLENECK
    // ... encode code ...
}
```

**Váº¥n Ä‘á»:**
- Global mutex serialize Táº¤T Cáº¢ PNG encoding
- Chá»‰ 1 thread encode PNG táº¡i 1 thá»i Ä‘iá»ƒm
- CÃ¡c thread khÃ¡c pháº£i chá» â†’ waste CPU cores

**Giáº£i phÃ¡p:**
```cpp
bool encode_png(...) {
    // NO MUTEX - libpng 1.6+ is thread-safe per png_struct
    // Each thread creates its own png_struct â†’ safe parallel encoding

    png_structp png = png_create_write_struct(...);  // Thread-local
    png_infop info = png_create_info_struct(png);
    // ... rest of code unchanged ...
}
```

**Verification cáº§n lÃ m:**
1. Check libpng version: `pkg-config --modversion libpng` (cáº§n â‰¥ 1.6.0)
2. Náº¿u < 1.6.0: thÃªm per-thread mutex thay vÃ¬ global mutex

**Testing:**
```bash
# Run PNG batch test
ruby bindings/ruby/test_all_features.rb
# Pháº£i PASS 100% - output files giá»‘ng há»‡t

# Benchmark PNG performance
cd build/benchmark
./bench_formats  # So sÃ¡nh PNG speed
```

**Expected result:**
- PNG batch: 40-60% faster
- Other formats: khÃ´ng áº£nh hÆ°á»Ÿng

---

### E1.2: Buffer Pool cho RGBAâ†’RGB Conversion (Impact: 20% faster)

**Files cáº§n sá»­a:**
- `src/encoder.cpp:34-124` (JPEG encoding)
- `src/internal.h` (buffer pool interface)

**Hiá»‡n táº¡i:**
```cpp
// encoder.cpp:76-90
if (data.channels == 4) {
    // Allocate NEW buffer má»—i láº§n
    rgb_buffer = new unsigned char[rgb_size];  // â† SLOW

    // Convert RGBA â†’ RGB
    for (int i = 0; i < data.width * data.height; i++) {
        *dst++ = *src++;  // R
        *dst++ = *src++;  // G
        *dst++ = *src++;  // B
        src++;            // Skip A
    }

    delete[] rgb_buffer;  // â† SLOW
}
```

**Váº¥n Ä‘á»:**
- Má»—i image allocate/deallocate buffer má»›i
- Vá»›i batch 500 images â†’ 500 láº§n allocate/free
- Memory allocator overhead Ä‘Ã¡ng ká»ƒ

**Giáº£i phÃ¡p:**
```cpp
// ThÃªm vÃ o ThreadPool - má»—i thread cÃ³ buffer pool riÃªng
struct ThreadLocalBuffers {
    std::vector<unsigned char*> rgb_buffers;
    size_t buffer_capacity;

    unsigned char* get_buffer(size_t size) {
        if (!rgb_buffers.empty() && buffer_capacity >= size) {
            auto buf = rgb_buffers.back();
            rgb_buffers.pop_back();
            return buf;
        }
        return new unsigned char[size];
    }

    void return_buffer(unsigned char* buf) {
        if (rgb_buffers.size() < 4) {  // Keep max 4 buffers per thread
            rgb_buffers.push_back(buf);
        } else {
            delete[] buf;
        }
    }
};

// encoder.cpp - sá»­ dá»¥ng buffer pool
if (data.channels == 4) {
    rgb_buffer = thread_local_buffers->get_buffer(rgb_size);
    // ... convert ...
    // Tráº£ láº¡i thay vÃ¬ delete
    thread_local_buffers->return_buffer(rgb_buffer);
}
```

**Testing:**
```bash
# Test vá»›i PNG â†’ JPG conversion (cÃ³ alpha channel)
ruby bindings/ruby/test_all_features.rb

# Benchmark RGBA images
cd build/benchmark
./bench_formats  # Test vá»›i PNG input â†’ JPG output
```

**Expected result:**
- Images vá»›i alpha channel: 15-20% faster
- RGB images: khÃ´ng áº£nh hÆ°á»Ÿng (khÃ´ng dÃ¹ng buffer pool)

---

### E1.3: Lower Pipeline Threshold (Impact: 25% faster cho batch nhá»)

**Files cáº§n sá»­a:**
- `src/fastresize.cpp:270`
- `src/fastresize.cpp:369`

**Hiá»‡n táº¡i:**
```cpp
// fastresize.cpp:270
if (batch_opts.max_speed && input_paths.size() >= 50) {
    // Use 3-stage pipeline
}
```

**Váº¥n Ä‘á»:**
- Pipeline chá»‰ cho batch â‰¥50
- Batch 10-49 dÃ¹ng thread pool cÅ© (cháº­m hÆ¡n)
- Pipeline architecture tá»‘t hÆ¡n nhÆ°ng khÃ´ng Ä‘Æ°á»£c dÃ¹ng

**Giáº£i phÃ¡p:**
```cpp
// Conservative threshold: 20 images
// Vá»›i batch >= 20, pipeline Ä‘á»§ hiá»‡u quáº£
if (batch_opts.max_speed && input_paths.size() >= 20) {
    // Use 3-stage pipeline
    // Queue capacity (32) Ä‘á»§ cho batch 20-50
}
```

**Rationale:**
- Queue capacity = 32 (pipeline.h:125)
- Vá»›i batch 20: queue util ~60% (ok)
- Vá»›i batch < 20: overhead > benefit (skip pipeline)

**Testing:**
```bash
# Test batch sizes: 10, 20, 30, 50
ruby bindings/ruby/benchmark_500.rb  # CÃ³ batch test

# Benchmark khÃ¡c nhau
cd build/benchmark
./bench_pipeline  # Test pipeline vá»›i batch sizes khÃ¡c nhau
```

**Expected result:**
- Batch 20-49: 20-30% faster
- Batch < 20: khÃ´ng áº£nh hÆ°á»Ÿng
- Batch â‰¥ 50: khÃ´ng áº£nh hÆ°á»Ÿng

---

### E1 Summary

**Changes:**
1. Remove PNG global mutex â†’ thread-safe parallel encoding
2. Buffer pool cho RGBAâ†’RGB â†’ reuse memory
3. Pipeline threshold 50â†’20 â†’ more batches use pipeline

**Expected improvements:**
- PNG batch: 40-60% faster
- JPEG batch (RGBA input): 15-20% faster
- Small batch (20-49): 20-30% faster
- **Overall: 50-70% faster**

**Risk level:** ğŸŸ¢ LOW
- KhÃ´ng thay Ä‘á»•i output
- Backward compatible 100%
- CÃ³ test coverage

**Validation:**
```bash
# 1. Run all tests
ruby bindings/ruby/test_all_features.rb
# Pháº£i: ALL TESTS PASSED

# 2. Run benchmark
./build/benchmark/bench_vs_libvips
# Expected: FastResize gáº§n báº±ng hoáº·c nhanh hÆ¡n libvips parallel

# 3. Visual check
# So sÃ¡nh output images trÆ°á»›c/sau optimization
# Pháº£i: BIT-IDENTICAL (hoáº·c PSNR > 50dB)
```

---

## ğŸ“‹ PHASE E2: DEEP OPTIMIZATION (Medium Impact, Medium Risk)

### Má»¥c tiÃªu
- **+40-50% faster** so vá»›i Phase E1
- Cáº§n testing ká»¹ hÆ¡n
- CÃ³ thá»ƒ thay Ä‘á»•i internal implementation

### E2.1: SIMD cho RGBAâ†’RGB Conversion (Impact: 15% faster)

**Files cáº§n sá»­a:**
- Táº¡o má»›i: `src/simd_convert.h`, `src/simd_convert.cpp`
- Sá»­a: `src/encoder.cpp:81-88`

**Hiá»‡n táº¡i:**
```cpp
// Scalar code - process 1 pixel má»—i láº§n
for (int i = 0; i < data.width * data.height; i++) {
    *dst++ = *src++;  // R
    *dst++ = *src++;  // G
    *dst++ = *src++;  // B
    src++;            // Skip A
}
```

**Váº¥n Ä‘á»:**
- Chá»‰ process 1 pixel/iteration
- CPU modern cÃ³ thá»ƒ process 16-32 pixels cÃ¹ng lÃºc (SIMD)
- Apple Silicon (NEON): 16 bytes/instruction
- x86 AVX2: 32 bytes/instruction

**Giáº£i phÃ¡p:**
```cpp
// NEON (Apple Silicon) - process 16 pixels at once
#ifdef __ARM_NEON
    size_t num_pixels = data.width * data.height;
    size_t simd_pixels = num_pixels - (num_pixels % 16);

    for (size_t i = 0; i < simd_pixels; i += 16) {
        // Load 16 RGBA pixels (64 bytes)
        uint8x16x4_t rgba = vld4q_u8(src);

        // Store RGB only (48 bytes) - drop A
        uint8x16x3_t rgb;
        rgb.val[0] = rgba.val[0];  // R
        rgb.val[1] = rgba.val[1];  // G
        rgb.val[2] = rgba.val[2];  // B
        vst3q_u8(dst, rgb);

        src += 64;
        dst += 48;
    }

    // Handle remaining pixels (< 16) with scalar code
    for (size_t i = simd_pixels; i < num_pixels; i++) {
        *dst++ = *src++;  // R
        *dst++ = *src++;  // G
        *dst++ = *src++;  // B
        src++;            // Skip A
    }
#else
    // Fallback: scalar code (current implementation)
#endif
```

**Platform support:**
- âœ… Apple Silicon (M1/M2/M3): NEON built-in
- âœ… x86_64: AVX2 (Intel 2013+, AMD 2015+)
- âœ… ARM Linux: NEON (Raspberry Pi 3+)
- âœ… Fallback: scalar code cho old CPUs

**Testing:**
```bash
# 1. Correctness test
ruby bindings/ruby/test_all_features.rb
# Test vá»›i PNG (RGBA) â†’ JPG conversion

# 2. Performance test
cd build/benchmark
./bench_formats  # RGBA images

# 3. Platform test
# macOS (NEON): should be faster
# Linux x86: AVX2 detection
```

**Expected result:**
- RGBAâ†’RGB: 10-15% faster
- RGB images: khÃ´ng áº£nh hÆ°á»Ÿng (khÃ´ng conversion)

---

### E2.2: Optimize JPEG Encoding vá»›i libjpeg-turbo (Impact: 25% faster)

**Files cáº§n sá»­a:**
- `src/encoder.cpp:34-124`

**Hiá»‡n táº¡i:**
```cpp
while (cinfo.next_scanline < cinfo.image_height) {
    row_pointer[0] = &encode_pixels[cinfo.next_scanline * row_stride];
    jpeg_write_scanlines(&cinfo, row_pointer, 1);  // 1 dÃ²ng/láº§n
}
```

**Váº¥n Ä‘á»:**
- Write 1 scanline má»—i láº§n â†’ nhiá»u function calls
- libjpeg-turbo cÃ³ thá»ƒ write nhiá»u scanlines cÃ¹ng lÃºc
- KhÃ´ng táº­n dá»¥ng háº¿t libjpeg-turbo optimization

**Giáº£i phÃ¡p 1: Batch scanlines**
```cpp
// Write 16 scanlines má»—i láº§n
const int SCANLINES_PER_BATCH = 16;
JSAMPROW row_pointers[SCANLINES_PER_BATCH];

while (cinfo.next_scanline < cinfo.image_height) {
    int rows_left = cinfo.image_height - cinfo.next_scanline;
    int rows_to_write = std::min(rows_left, SCANLINES_PER_BATCH);

    for (int i = 0; i < rows_to_write; i++) {
        row_pointers[i] = &encode_pixels[(cinfo.next_scanline + i) * row_stride];
    }

    jpeg_write_scanlines(&cinfo, row_pointers, rows_to_write);
}
```

**Giáº£i phÃ¡p 2: libjpeg-turbo specific optimizations**
```cpp
// Enable libjpeg-turbo specific features
#ifdef LIBJPEG_TURBO_VERSION
    // Use SIMD-optimized DCT
    cinfo.dct_method = JDCT_IFAST;  // Already done in Phase A

    // Optimize for batch encoding
    cinfo.optimize_coding = FALSE;  // Faster, slightly larger files

    // Disable smoothing (faster, minimal quality impact)
    cinfo.smoothing_factor = 0;
#endif
```

**Testing:**
```bash
# Quality test - ensure output quality ok
cd bindings/ruby
ruby test_all_features.rb

# Performance test
cd build/benchmark
./bench_formats  # JPEG encoding speed

# Quality comparison (PSNR should be > 40dB)
# Compare before/after vá»›i ImageMagick
compare -metric PSNR before.jpg after.jpg diff.png
```

**Expected result:**
- JPEG encoding: 20-30% faster
- Quality: PSNR > 40dB (acceptable)
- File size: +2-5% larger (acceptable trade-off)

---

### E2.3: Better Thread Pool Work Distribution (Impact: 10% faster)

**Files cáº§n sá»­a:**
- `src/thread_pool.cpp`
- `src/fastresize.cpp:304-343`

**Hiá»‡n táº¡i:**
```cpp
// Simple work queue - FIFO
for (const std::string& input_path : input_paths) {
    thread_pool_enqueue(pool, [&, input_path]() {
        // Process image
    });
}
```

**Váº¥n Ä‘á»:**
- Táº¥t cáº£ images Ä‘Æ°á»£c treat nhÆ° nhau
- Large images (5MB) vÃ  small images (500KB) cÃ¹ng queue
- Thread process small image xong pháº£i chá» thread khÃ¡c (large image)
- Load imbalance â†’ waste CPU time

**Giáº£i phÃ¡p: Work stealing vá»›i size-based priority**
```cpp
// Sort images by size (largest first)
struct ImageTask {
    std::string path;
    size_t file_size;
};

std::vector<ImageTask> tasks;
for (const auto& path : input_paths) {
    struct stat st;
    stat(path.c_str(), &st);
    tasks.push_back({path, st.st_size});
}

// Sort by size DESC - process large images first
std::sort(tasks.begin(), tasks.end(),
    [](const ImageTask& a, const ImageTask& b) {
        return a.file_size > b.file_size;
    });

// Enqueue sorted tasks
for (const auto& task : tasks) {
    thread_pool_enqueue(pool, [&, task]() {
        // Process image
    });
}
```

**Rationale:**
- Large images first â†’ better load balancing
- Khi large images xong, threads cÃ¹ng finish small images
- Giáº£m tail latency (thá»i gian thread cuá»‘i cÃ¹ng)

**Testing:**
```bash
# Test vá»›i mixed image sizes
cd bindings/ruby
ruby benchmark_500.rb  # Mixed sizes

# Check CPU utilization
# TrÆ°á»›c: uneven (some threads idle)
# Sau: even (all threads busy)
```

**Expected result:**
- Better CPU utilization
- 5-15% faster (depending on image size variance)

---

### E2 Summary

**Changes:**
1. SIMD RGBAâ†’RGB â†’ 16-32 pixels/instruction
2. Optimize JPEG encoding â†’ batch scanlines
3. Better work distribution â†’ large images first

**Expected improvements:**
- RGBA conversion: 10-15% faster
- JPEG encoding: 20-30% faster
- Load balancing: 5-15% faster
- **Overall: +40-50% faster than E1**

**Risk level:** ğŸŸ¡ MEDIUM
- SIMD cáº§n fallback cho old CPUs
- JPEG optimization cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng quality (cáº§n verify)
- Work distribution cáº§n testing vá»›i mixed sizes

**Validation:**
```bash
# 1. Quality test
ruby bindings/ruby/test_all_features.rb

# 2. SIMD correctness
# So sÃ¡nh output SIMD vs scalar â†’ pháº£i giá»‘ng há»‡t

# 3. Benchmark
./build/benchmark/bench_vs_libvips
# Expected: FastResize nhanh hÆ¡n libvips parallel 10-20%
```

---

## ğŸ“‹ PHASE E3: ADVANCED TUNING (Low Impact, Higher Risk)

### Má»¥c tiÃªu
- **+10-20% faster** so vá»›i Phase E2
- Advanced optimizations
- CÃ³ thá»ƒ cÃ³ trade-offs (memory, compatibility)

### E3.1: Memory-mapped I/O (Impact: 10% faster)

**Files cáº§n sá»­a:**
- Táº¡o má»›i: `src/io_mmap.h`, `src/io_mmap.cpp`
- Sá»­a: `src/decoder.cpp`, `src/encoder.cpp`

**Hiá»‡n táº¡i:**
```cpp
// decoder.cpp - sá»­ dá»¥ng fread()
FILE* fp = fopen(path.c_str(), "rb");
fread(buffer, 1, size, fp);  // Copy tá»« kernel â†’ userspace
```

**Váº¥n Ä‘á»:**
- `fread()` copy data tá»« kernel space â†’ user space
- Má»—i image: 1 copy operation
- Batch 500 images: 500 copy operations

**Giáº£i phÃ¡p:**
```cpp
// mmap - zero-copy I/O
int fd = open(path.c_str(), O_RDONLY);
struct stat st;
fstat(fd, &st);

void* mapped = mmap(NULL, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
// mapped points directly to kernel page cache
// NO COPY - decode directly tá»« mmap memory

// Decode...
munmap(mapped, st.st_size);
close(fd);
```

**Caveats:**
- mmap cÃ³ thá»ƒ fail vá»›i network drives
- Very large files (>2GB) trÃªn 32-bit cÃ³ váº¥n Ä‘á»
- Cáº§n fallback to fread()

**Implementation:**
```cpp
bool decode_with_mmap(const std::string& path, ImageData& data) {
    int fd = open(path.c_str(), O_RDONLY);
    if (fd < 0) return false;

    struct stat st;
    if (fstat(fd, &st) < 0) {
        close(fd);
        return false;
    }

    // Skip mmap for very large files or network drives
    if (st.st_size > 100 * 1024 * 1024) {  // > 100MB
        close(fd);
        return false;  // Fallback to fread
    }

    void* mapped = mmap(NULL, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
    if (mapped == MAP_FAILED) {
        close(fd);
        return false;  // Fallback to fread
    }

    // Decode tá»« mmap memory
    // ...

    munmap(mapped, st.st_size);
    close(fd);
    return true;
}
```

**Testing:**
```bash
# Test vá»›i files khÃ¡c nhau
ruby bindings/ruby/test_all_features.rb

# Test edge cases
# - Very large files
# - Network drives
# - Permission issues

# Benchmark
cd build/benchmark
./bench_vs_libvips
```

**Expected result:**
- Local files: 5-10% faster
- Network files: fallback to fread (no regression)

---

### E3.2: Pipeline Tuning (Impact: 5% faster)

**Files cáº§n sá»­a:**
- `src/pipeline.cpp`
- `src/pipeline.h`

**Hiá»‡n táº¡i:**
```cpp
// pipeline.h:370
PipelineProcessor pipeline(4, 8, 4, 32);
// decode_threads=4, resize_threads=8, encode_threads=4, queue=32
```

**Váº¥n Ä‘á»:**
- Fixed thread counts (4-8-4)
- KhÃ´ng adapt to workload
- JPEG-heavy vs PNG-heavy cÃ³ optimal khÃ¡c nhau

**Giáº£i phÃ¡p: Dynamic thread allocation**
```cpp
// Detect workload characteristics
size_t jpeg_count = 0, png_count = 0, webp_count = 0;
for (const auto& item : items) {
    auto fmt = detect_format(item.input_path);
    if (fmt == FORMAT_JPEG) jpeg_count++;
    else if (fmt == FORMAT_PNG) png_count++;
    else if (fmt == FORMAT_WEBP) webp_count++;
}

// Adjust thread counts based on workload
size_t decode_threads, resize_threads, encode_threads;

if (jpeg_count > items.size() * 0.8) {
    // JPEG-heavy: encoding is fast, more decode threads
    decode_threads = 6;
    resize_threads = 8;
    encode_threads = 2;
} else if (png_count > items.size() * 0.8) {
    // PNG-heavy: encoding is slow, more encode threads
    decode_threads = 3;
    resize_threads = 8;
    encode_threads = 5;
} else {
    // Mixed: balanced
    decode_threads = 4;
    resize_threads = 8;
    encode_threads = 4;
}

PipelineProcessor pipeline(decode_threads, resize_threads,
                           encode_threads, 32);
```

**Testing:**
```bash
# Test vá»›i workloads khÃ¡c nhau
# 1. 100% JPEG
# 2. 100% PNG
# 3. Mixed (50/50)

cd build/benchmark
./bench_pipeline
```

**Expected result:**
- JPEG-heavy: 5-10% faster
- PNG-heavy: 5-10% faster
- Mixed: ~5% faster

---

### E3.3: Prefetch Optimization (Impact: 5% faster)

**Files cáº§n sá»­a:**
- `src/pipeline.cpp:40-78` (decode stage)

**Hiá»‡n táº¡i:**
```cpp
// Sequential processing
for (size_t i = 0; i < items.size(); ++i) {
    decode_task(items[i]);  // Load tá»« disk khi cáº§n
}
```

**Váº¥n Ä‘á»:**
- Wait for disk I/O má»—i task
- CPU idle trong khi chá» I/O
- KhÃ´ng táº­n dá»¥ng OS page cache

**Giáº£i phÃ¡p: Prefetch next N images**
```cpp
// Prefetch using posix_fadvise
for (size_t i = 0; i < items.size(); ++i) {
    // Prefetch next 3 images
    for (size_t j = i + 1; j < std::min(i + 4, items.size()); ++j) {
        int fd = open(items[j].input_path.c_str(), O_RDONLY);
        posix_fadvise(fd, 0, 0, POSIX_FADV_WILLNEED);  // Hint to OS
        close(fd);
    }

    // Process current image
    decode_task(items[i]);
}
```

**Testing:**
```bash
# Test vá»›i cold vs warm cache
echo 3 > /proc/sys/vm/drop_caches  # Linux only

cd build/benchmark
./bench_vs_libvips
```

**Expected result:**
- Cold cache: 10-15% faster
- Warm cache: 0-5% faster (already in cache)

---

### E3 Summary

**Changes:**
1. mmap I/O â†’ zero-copy reads
2. Dynamic pipeline tuning â†’ adapt to workload
3. Prefetch optimization â†’ hide I/O latency

**Expected improvements:**
- I/O bound: 5-10% faster
- Mixed workloads: 5-10% faster
- **Overall: +10-20% faster than E2**

**Risk level:** ğŸŸ¡ MEDIUM-HIGH
- mmap cáº§n fallback cáº©n tháº­n
- Dynamic tuning cÃ³ thá»ƒ backfire vá»›i edge cases
- Prefetch cÃ³ thá»ƒ waste memory vá»›i large files

**Validation:**
```bash
# Full test suite
ruby bindings/ruby/test_all_features.rb

# Stress test vá»›i edge cases
# - Very large files (>100MB)
# - Many small files (<10KB)
# - Network drives
# - Low memory conditions

# Final benchmark
./build/benchmark/bench_vs_libvips
# Expected: FastResize nhanh hÆ¡n libvips parallel 20-30%
```

---

## ğŸ¯ Tá»”NG Káº¾T PHASE E

### Performance Targets

| Phase | Optimization | Expected Gain | Cumulative | vs libvips |
|-------|-------------|---------------|------------|------------|
| **Baseline** | Current | 0% | 1.0x | Slower |
| **E1** | Quick Wins | +50-70% | 1.6x | ~Equal |
| **E2** | Deep Opt | +40-50% | 2.4x | +20% faster |
| **E3** | Advanced | +10-20% | 2.8x | +30% faster |

### Features Guarantee

âœ… **100% Backward Compatible:**
- JPG, PNG, BMP, WEBP support
- Single + Batch resize
- All resize modes (%, width, height, exact)
- Custom params per image
- Quality settings
- Filter options

### Testing Strategy

**After each phase:**
```bash
# 1. Correctness
ruby bindings/ruby/test_all_features.rb
# Must: ALL TESTS PASSED âœ…

# 2. Performance
./build/benchmark/bench_vs_libvips
# Track: Time, Throughput, per-image latency

# 3. Quality
# Visual inspection + PSNR check
# Must: PSNR > 40dB, visually identical
```

### Benchmark Command

```bash
# Build optimized
cmake -B build -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_CXX_FLAGS="-O3 -march=native"
cmake --build build -j8

# Run benchmark
cd /Users/canh.th/Desktop/fastgems/fastresize
./build/benchmark/bench_vs_libvips

# Expected output format:
# ============================================================
# COMPARISON
# ============================================================
# Library           Time(s)  Throughput   Per Image
# ------------------------------------------------------------
# FastResize (E1)    4.23    118.5/s      8.43ms     â† Phase E1
# FastResize (E2)    2.95    169.5/s      5.90ms     â† Phase E2
# FastResize (E3)    2.50    200.0/s      5.00ms     â† Phase E3
# libvips parallel   3.80    131.6/s      7.60ms
#
# ğŸ† Winner: FastResize (E3)
#    34% faster than libvips parallel
```

---

## ğŸ“ KHI TAG VÃ€O FILE NÃ€Y

### Phase E1: Quick Wins
**Tag format:** `@claude implement E1.1` hoáº·c `@claude start Phase E1`

**TÃ´i sáº½ lÃ m:**
1. âœ… Read file nÃ y Ä‘á»ƒ hiá»ƒu context
2. âœ… Implement E1.1 (PNG mutex), E1.2 (buffer pool), E1.3 (pipeline threshold)
3. âœ… Run tests: `ruby test_all_features.rb`
4. âœ… Run benchmark: `./build/benchmark/bench_vs_libvips`
5. âœ… Report results: performance gain, pass/fail tests
6. âœ… Commit changes vá»›i detailed message

### Phase E2: Deep Optimization
**Tag format:** `@claude implement E2.1` hoáº·c `@claude start Phase E2`

**TÃ´i sáº½ lÃ m:**
1. âœ… Implement SIMD, JPEG opt, work distribution
2. âœ… Test correctness + performance
3. âœ… Benchmark so sÃ¡nh vá»›i E1
4. âœ… Report & commit

### Phase E3: Advanced Tuning
**Tag format:** `@claude implement E3` hoáº·c `@claude start Phase E3`

**TÃ´i sáº½ lÃ m:**
1. âœ… Implement mmap, dynamic tuning, prefetch
2. âœ… Test vá»›i edge cases
3. âœ… Final benchmark vs libvips
4. âœ… Summary report vá»›i charts

### Benchmark After Each Phase
**Tag format:** `@claude benchmark E1` hoáº·c `@claude compare performance`

**TÃ´i sáº½ lÃ m:**
1. âœ… Build release binary
2. âœ… Run `bench_vs_libvips`
3. âœ… Parse results
4. âœ… Generate comparison table
5. âœ… Analyze bottlenecks if any

---

## ğŸš€ READY TO START

**Recommended order:**
1. `@claude start Phase E1` - Quick wins, low risk
2. `@claude benchmark E1` - Verify gains
3. `@claude start Phase E2` - Deep optimization
4. `@claude benchmark E2` - Verify gains
5. `@claude start Phase E3` - Advanced tuning
6. `@claude benchmark E3` - Final comparison

**Estimated timeline:**
- E1: 2-3 days (worth 50-70% gain) âš¡âš¡âš¡
- E2: 3-5 days (worth 40-50% gain) âš¡âš¡
- E3: 5-7 days (worth 10-20% gain) âš¡

**Total: ~2 weeks Ä‘á»ƒ Ä‘áº¡t 2-3x faster than current, tháº¯ng libvips**

---

## ğŸ“Š SUCCESS CRITERIA

### Phase E1 Success
- [ ] Tests pass 100%
- [ ] PNG batch: â‰¥40% faster
- [ ] JPEG (RGBA): â‰¥15% faster
- [ ] Overall: â‰¥50% faster
- [ ] vs libvips: ~equal or faster

### Phase E2 Success
- [ ] Tests pass 100%
- [ ] SIMD working on Apple Silicon
- [ ] JPEG: â‰¥20% faster than E1
- [ ] Overall: â‰¥40% faster than E1
- [ ] vs libvips: â‰¥15% faster

### Phase E3 Success
- [ ] Tests pass 100%
- [ ] Edge cases handled (mmap fallback, etc)
- [ ] Overall: â‰¥10% faster than E2
- [ ] vs libvips: â‰¥25% faster
- [ ] **Memory usage stable** (no leaks)

### Final Success
- [ ] **FastResize â‰¥2.5x faster than baseline**
- [ ] **FastResize â‰¥20% faster than libvips parallel**
- [ ] 100% backward compatible
- [ ] All tests passing
- [ ] No memory leaks (valgrind clean)
- [ ] Production ready âœ…

---

**Version:** 1.0
**Created:** 2025-12-03
**Status:** Ready for implementation
**Contact:** Tag @claude with phase name to start
